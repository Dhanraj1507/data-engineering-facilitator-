# InUnity Data Engineering Facilitator Task

## 🔧 Tools Used
- PySpark
- Spark SQL
- Google Colab
- Parquet Format

## 📂 Folder Structure
- `notebook/`: ETL pipeline in PySpark
- `sql/`: SQL queries
- `docs/`: PDFs for curriculum, feedback, conceptual answers
- `output/`: (Optional) screenshots, output CSVs

## 🧱 Pipeline Overview
- Load and clean raw sales data
- Aggregations:
  - Monthly Sales by Region
  - Top 10 Customers by Profit
  - Average Discount per Category
- SQL Reporting using Spark SQL
- Output stored as partitioned Parquet

## 🖼️ Sample Output
_Add screenshots or CSV if possible_  
E.g., Monthly Sales Parquet → sample_output.csv

## 🚀 How to Run (Google Colab)
1. Open `pipeline.ipynb` in Google Colab
2. Upload your `sales.csv` file
3. Run all cells to clean, analyze, and export parquet
4. (Optional) View SQL outputs via notebook cells

## 👨‍💻 Author
Dhanraj Tiwari
