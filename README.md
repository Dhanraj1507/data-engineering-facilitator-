# InUnity Data Engineering Facilitator Task

## ğŸ”§ Tools Used
- PySpark
- Spark SQL
- Google Colab
- Parquet Format

## ğŸ“‚ Folder Structure
- `notebook/`: ETL pipeline in PySpark
- `sql/`: SQL queries
- `docs/`: PDFs for curriculum, feedback, conceptual answers
- `output/`: (Optional) screenshots, output CSVs

## ğŸ§± Pipeline Overview
- Load and clean raw sales data
- Aggregations:
  - Monthly Sales by Region
  - Top 10 Customers by Profit
  - Average Discount per Category
- SQL Reporting using Spark SQL
- Output stored as partitioned Parquet

## ğŸ–¼ï¸ Sample Output
_Add screenshots or CSV if possible_  
E.g., Monthly Sales Parquet â†’ sample_output.csv

## ğŸš€ How to Run (Google Colab)
1. Open `pipeline.ipynb` in Google Colab
2. Upload your `sales.csv` file
3. Run all cells to clean, analyze, and export parquet
4. (Optional) View SQL outputs via notebook cells

## ğŸ‘¨â€ğŸ’» Author
Dhanraj Tiwari
